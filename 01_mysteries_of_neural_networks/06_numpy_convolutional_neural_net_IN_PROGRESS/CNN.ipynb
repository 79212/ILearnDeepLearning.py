{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, Optional, Union, List\n",
    "from enum import Enum\n",
    "from abc import ABC, abstractmethod\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
    "    return e_Z / e_Z.sum(axis = 1, keepdims=True)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Enum):\n",
    "    RELU = (\"relu\", relu, relu_backward)\n",
    "    SIGMOID = (\"sigmoid\", sigmoid, sigmoid_backward)\n",
    "    SOFTMAX = (\"softmax\", softmax, lambda dA_curr, Z_curr: dA_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def build(self, input_shape: tuple) -> tuple:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward_pass(self, input: np.array) -> np.array:\n",
    "        pass\n",
    "        \n",
    "    @abstractmethod\n",
    "    def backward_pass(self, input: np.array) -> np.array:\n",
    "        pass\n",
    "    \n",
    "    def _init_weights(self, shape: tuple) -> np.array:\n",
    "        return np.random.normal(0.0, 0.1, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    def __init__(self, \n",
    "                 filters: int, \n",
    "                 kernel_size: Tuple[int, int, int], \n",
    "                 padding: int, \n",
    "                 stride: int, \n",
    "                 activation: Activation):\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape: tuple) -> tuple:\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, input: np.array) -> np.array:\n",
    "        pass\n",
    "    \n",
    "    def backward_pass(self, input: np.array) -> np.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape: tuple) -> tuple:\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, input: np.array) -> np.array:\n",
    "        pass\n",
    "    \n",
    "    def backward_pass(self, input: np.array) -> np.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAveragePool(Layer):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build(self, input_shape: tuple) -> tuple:\n",
    "        pass\n",
    "    \n",
    "    def forward_pass(self, input: np.array) -> np.array:\n",
    "        pass\n",
    "    \n",
    "    def backward_pass(self, input: np.array) -> np.array:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Uses the Adam update rule, which incorporates moving averages of both the\n",
    "    gradient and its square and a bias correction term.\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - beta1: Decay rate for moving average of first moment of gradient.\n",
    "    - beta2: Decay rate for moving average of second moment of gradient.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - m: Moving average of gradient.\n",
    "    - v: Moving average of squared gradient.\n",
    "    - t: Iteration number.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-3)\n",
    "    config.setdefault('beta1', 0.9)\n",
    "    config.setdefault('beta2', 0.999)\n",
    "    config.setdefault('epsilon', 1e-8)\n",
    "    config.setdefault('m', np.zeros_like(w))\n",
    "    config.setdefault('v', np.zeros_like(w))\n",
    "    config.setdefault('t', 0)\n",
    "\n",
    "    next_w = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the Adam update formula, storing the next value of w in #\n",
    "    # the next_w variable. Don't forget to update the m, v, and t variables   #\n",
    "    # stored in config.                                                       #\n",
    "    #                                                                         #\n",
    "    # NOTE: In order to match the reference output, please modify t _before_  #\n",
    "    # using it in any calculations.                                           #\n",
    "    ###########################################################################\n",
    "    eps, learning_rate = config['epsilon'], config['learning_rate']\n",
    "    beta1, beta2 = config['beta1'], config['beta2']\n",
    "    m, v, t = config['m'], config['v'], config['t']\n",
    "    # Adam\n",
    "    t = t + 1\n",
    "    m = beta1 * m + (1 - beta1) * dw          # momentum\n",
    "    mt = m / (1 - beta1**t)                   # bias correction\n",
    "    v = beta2 * v + (1 - beta2) * (dw * dw)   # RMSprop\n",
    "    vt = v / (1 - beta2**t)                   # bias correction\n",
    "    next_w = w - learning_rate * mt / (np.sqrt(vt) + eps)\n",
    "    # update values\n",
    "    config['m'], config['v'], config['t'] = m, v, t\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, \n",
    "                 units: int, \n",
    "                 activation: Activation):\n",
    "        \n",
    "        self._units = units\n",
    "        self._activation = activation\n",
    "        \n",
    "        self._A = None\n",
    "        self._W = None\n",
    "        self._b = None\n",
    "        self._Z = None\n",
    "        \n",
    "        self._dA = None\n",
    "        self._dW = None\n",
    "        self._db = None\n",
    "        self._dZ = None\n",
    "        \n",
    "        self._W_config = None\n",
    "        self._b_config = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Dense: {: <25} {: <25} {: <25}\".format(\n",
    "            \"W.shape={}\".format(self._W.shape), \n",
    "            \"b.shape={}\".format(self._b.shape), \n",
    "            \"activation={}\".format(self._activation.value[0])\n",
    "        )\n",
    "    \n",
    "    def build(self, input_shape: tuple) -> tuple:\n",
    "        self._W = self._init_weights((input_shape[1], self._units))\n",
    "        self._b = self._init_weights((self._units,))\n",
    "        return (None, self._units)\n",
    "    \n",
    "    def forward_pass(self, input: np.array) -> np.array:\n",
    "        self._A = input\n",
    "        \n",
    "        self._Z = input.dot(self._W) + self._b\n",
    "        return self._activation.value[1](self._Z)\n",
    "    \n",
    "    def backward_pass(self, input: np.array) -> np.array:\n",
    "        N = input.shape[0]\n",
    "        self._dZ = self._activation.value[2](input, self._Z)\n",
    "        \n",
    "        self._dW = self._A.T.dot(self._dZ)\n",
    "        self._db = self._dZ.sum(axis=0)\n",
    "        self._dA = self._dZ.dot(self._W.T) \n",
    "        return self._dA\n",
    "    \n",
    "    def update(self, lr: float) -> None:\n",
    "#         self._W -= lr * self._dW\n",
    "#         self._b -= lr * self._db\n",
    "        \n",
    "        self._W, self._W_config = adam(self._W, self._dW, self._W_config)\n",
    "        self._b, self._b_config = adam(self._b, self._db, self._b_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, layers: Layer):\n",
    "        \n",
    "        self._layers = layers\n",
    "        self._compiled = False\n",
    "    \n",
    "    def build(self, input_shape: tuple) -> None:\n",
    "        shape = input_shape\n",
    "        for layer in self._layers:\n",
    "            shape = layer.build(shape)\n",
    "        self._compiled = True\n",
    "            \n",
    "    def summary(self):\n",
    "        if not self._compiled:\n",
    "            return\n",
    "        \n",
    "        for layer in self._layers:\n",
    "            print(layer)\n",
    "            print('-' * 80)\n",
    "            \n",
    "    def fit(self, X: np.array, y: np.array, epochs: int, lr: float, batch_size: int) -> None:\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            Y_hat = self._forward_pass(X=X)\n",
    "            loss, grads = self.softmax_loss(Y_hat, y)\n",
    "            print(\"LOSS:\", loss)\n",
    "            print(\"ACC:\", self._multi_class_accuracy(\n",
    "                Y_hat, y))\n",
    "\n",
    "            self._backward_pass(grads=grads)\n",
    "            self._update(lr=lr)\n",
    "            \n",
    "    def _forward_pass(self, X: np.array) -> np.array:\n",
    "        activations = X\n",
    "        for layer in self._layers: \n",
    "            activations = layer.forward_pass(input=activations)\n",
    "        return activations\n",
    "    \n",
    "    def _backward_pass(self, grads: np.array) -> None:\n",
    "        activations = grads\n",
    "        for layer in reversed(self._layers): \n",
    "            activations = layer.backward_pass(input=activations)\n",
    "            \n",
    "    def _update(self, lr: float) -> None:\n",
    "        for layer in self._layers: \n",
    "            layer.update(lr=lr)\n",
    "    \n",
    "    def _multi_class_cross_entropy_loss(self, Y_hat, Y):\n",
    "        N = Y_hat.shape[0]\n",
    "        Y = one_hot_encoding(Y)\n",
    "        loss = - np.sum(np.log(Y_hat) * Y) / N\n",
    "        return loss\n",
    "    \n",
    "    def _multi_class_accuracy(self, Y_hat, Y):\n",
    "        n_values = Y_hat.shape[1]\n",
    "        values = Y_hat.argmax(axis=1)\n",
    "        Y_hat_one_hot = np.eye(n_values)[values]\n",
    "        return (Y_hat_one_hot == one_hot_encoding(Y)).all(axis=1).mean()\n",
    "    \n",
    "    def svm_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the loss and gradient using for multiclass SVM classification.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "          class for the ith input.\n",
    "        - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "          0 <= y[i] < C\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar giving the loss\n",
    "        - dx: Gradient of the loss with respect to x\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        correct_class_scores = x[np.arange(N), y]\n",
    "        margins = np.maximum(0, x - correct_class_scores[:, np.newaxis] + 1.0)\n",
    "        margins[np.arange(N), y] = 0\n",
    "        loss = np.sum(margins) / N\n",
    "        num_pos = np.sum(margins > 0, axis=1)\n",
    "        dx = np.zeros_like(x)\n",
    "        dx[margins > 0] = 1\n",
    "        dx[np.arange(N), y] -= num_pos\n",
    "        dx /= N\n",
    "        return loss, dx\n",
    "\n",
    "    \n",
    "    def softmax_loss(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the loss and gradient for softmax classification.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "          class for the ith input.\n",
    "        - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "          0 <= y[i] < C\n",
    "\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar giving the loss\n",
    "        - dx: Gradient of the loss with respect to x\n",
    "        \"\"\"\n",
    "        probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "        N = x.shape[0]\n",
    "        loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n",
    "        dx = probs.copy()\n",
    "        dx[np.arange(N), y] -= 1\n",
    "        dx /= N\n",
    "        return loss, dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in the data set\n",
    "N_SAMPLES = 1000\n",
    "# ratio between training and test sets\n",
    "TEST_SIZE = 0.1\n",
    "# size of the photo\n",
    "PHOTO_SIZE = 28\n",
    "# number of pixels in the photo\n",
    "PIXEL_NUMBER = PHOTO_SIZE * PHOTO_SIZE\n",
    "# number of train epochs\n",
    "EPOCHS = 1000\n",
    "# learning rate value\n",
    "LR = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist_dataset():\n",
    "    # The MNIST data set is available at http://yann.lecun.com, let's use curl to download it\n",
    "    if not os.path.exists(\"train-images-idx3-ubyte\"):\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    "        !curl -O http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "        !gunzip t*-ubyte.gz\n",
    "        \n",
    "    # Let's use loadlocal_mnist available in mlxtend.data to get data in numpy array form.\n",
    "    X1, y1 = loadlocal_mnist(\n",
    "        images_path=\"train-images-idx3-ubyte\", \n",
    "        labels_path=\"train-labels-idx1-ubyte\")\n",
    "\n",
    "    X2, y2 = loadlocal_mnist(\n",
    "        images_path=\"t10k-images-idx3-ubyte\", \n",
    "        labels_path=\"t10k-labels-idx1-ubyte\")\n",
    "    \n",
    "    # We normalize the brightness values for pixels\n",
    "    X1 = X1.reshape(X1.shape[0], -1) / 255\n",
    "    X2 = X2.reshape(X2.shape[0], -1) /255\n",
    "\n",
    "    # Combine downloaded data bundles\n",
    "    X = np.concatenate([X1, X2])\n",
    "    y = np.concatenate([y1, y2])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(y):\n",
    "    n_values = np.max(y) + 1\n",
    "    return np.eye(n_values)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(split_percentage):\n",
    "    # Download data\n",
    "    X, y = download_mnist_dataset()\n",
    "    # One hot encode labels\n",
    "#     y = one_hot_encoding(y)\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=split_percentage, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = prepare_data(TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:2000]\n",
    "y_train = y_train[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 784)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense: W.shape=(784, 1000)       b.shape=(1000,)           activation=relu          \n",
      "--------------------------------------------------------------------------------\n",
      "Dense: W.shape=(1000, 1000)      b.shape=(1000,)           activation=relu          \n",
      "--------------------------------------------------------------------------------\n",
      "Dense: W.shape=(1000, 500)       b.shape=(500,)            activation=relu          \n",
      "--------------------------------------------------------------------------------\n",
      "Dense: W.shape=(500, 500)        b.shape=(500,)            activation=relu          \n",
      "--------------------------------------------------------------------------------\n",
      "Dense: W.shape=(500, 10)         b.shape=(10,)             activation=relu          \n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Dense(units=1000, activation=Activation.RELU),\n",
    "    Dense(units=1000, activation=Activation.RELU),\n",
    "    Dense(units=500, activation=Activation.RELU),\n",
    "    Dense(units=500, activation=Activation.RELU),\n",
    "    Dense(units=10, activation=Activation.RELU)\n",
    "]\n",
    "\n",
    "model = Model(layers=layers)\n",
    "model.build(input_shape=(None, PIXEL_NUMBER))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 8.474195808943529\n",
      "ACC: 0.11\n",
      "LOSS: 7.006311204467037\n",
      "ACC: 0.1695\n",
      "LOSS: 5.853604175800638\n",
      "ACC: 0.1675\n",
      "LOSS: 2.1801095140868263\n",
      "ACC: 0.19\n",
      "LOSS: 2.125027924504818\n",
      "ACC: 0.1865\n",
      "LOSS: 2.137466892511135\n",
      "ACC: 0.184\n",
      "LOSS: 2.1169416190799946\n",
      "ACC: 0.1875\n",
      "LOSS: 2.093895004847934\n",
      "ACC: 0.1975\n",
      "LOSS: 2.0791669939985225\n",
      "ACC: 0.2035\n",
      "LOSS: 2.0837937535793825\n",
      "ACC: 0.208\n",
      "LOSS: 2.0874939754544486\n",
      "ACC: 0.2085\n",
      "LOSS: 2.073972188238211\n",
      "ACC: 0.2085\n",
      "LOSS: 2.065692435748252\n",
      "ACC: 0.207\n",
      "LOSS: 2.0659438655877596\n",
      "ACC: 0.205\n",
      "LOSS: 2.069286290287001\n",
      "ACC: 0.2045\n",
      "LOSS: 2.067308884765191\n",
      "ACC: 0.2045\n",
      "LOSS: 2.061453864756405\n",
      "ACC: 0.2055\n",
      "LOSS: 2.0588426846840955\n",
      "ACC: 0.207\n",
      "LOSS: 2.05783896907339\n",
      "ACC: 0.208\n",
      "LOSS: 2.0572343471839343\n",
      "ACC: 0.21\n",
      "LOSS: 2.057010397971159\n",
      "ACC: 0.211\n",
      "LOSS: 2.055258094846513\n",
      "ACC: 0.211\n",
      "LOSS: 2.051653668377312\n",
      "ACC: 0.211\n",
      "LOSS: 2.050163015334989\n",
      "ACC: 0.2105\n",
      "LOSS: 2.050206136045861\n",
      "ACC: 0.2105\n",
      "LOSS: 2.0491492708668404\n",
      "ACC: 0.211\n",
      "LOSS: 2.0469387795304423\n",
      "ACC: 0.2115\n",
      "LOSS: 2.045813896634883\n",
      "ACC: 0.2115\n",
      "LOSS: 2.0466541030509884\n",
      "ACC: 0.2115\n",
      "LOSS: 2.046238735486512\n",
      "ACC: 0.212\n",
      "LOSS: 2.0452059406077057\n",
      "ACC: 0.2125\n",
      "LOSS: 2.043884294930947\n",
      "ACC: 0.2125\n",
      "LOSS: 2.04287420688582\n",
      "ACC: 0.2125\n",
      "LOSS: 2.042350885703152\n",
      "ACC: 0.213\n",
      "LOSS: 2.0408299846543407\n",
      "ACC: 0.2135\n",
      "LOSS: 2.0405715365653667\n",
      "ACC: 0.2135\n",
      "LOSS: 2.040491196611369\n",
      "ACC: 0.2135\n",
      "LOSS: 2.040251264310314\n",
      "ACC: 0.2135\n",
      "LOSS: 2.040247115285688\n",
      "ACC: 0.2135\n",
      "LOSS: 2.040370032078371\n",
      "ACC: 0.2135\n",
      "LOSS: 2.0404385597657395\n",
      "ACC: 0.2135\n",
      "LOSS: 2.040368949378792\n",
      "ACC: 0.2135\n",
      "LOSS: 2.0402544314503026\n",
      "ACC: 0.2135\n",
      "LOSS: 2.0397213542335875\n",
      "ACC: 0.214\n",
      "LOSS: 2.039451436686176\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0393228150056504\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0383844373081925\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03805382949897\n",
      "ACC: 0.2145\n",
      "LOSS: 2.038109323633397\n",
      "ACC: 0.2145\n",
      "LOSS: 2.038172527286535\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0381197250461858\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0379850835953928\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037923872696049\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037966592320858\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037978667495867\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037911149608512\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378259543212063\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037823117009805\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378252046470733\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037825671118268\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378244971601007\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378219674753653\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378185530987767\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037814761784642\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037811006113552\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037807544401746\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03780567973435\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03780496138331\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037801547579324\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0378004665930245\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03779940907573\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377984023921267\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037797464995241\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377966050801324\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037795825802667\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377951258440414\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377945003185243\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037793943821227\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037793449342064\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377930103017814\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377926205412633\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377929405751325\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792169905143\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792060189563\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037791946771747\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377918310065954\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377917141849453\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792326035344\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377917776837955\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377919333805186\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792062960544\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377921656017457\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377922412101865\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792290365381\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377923142402574\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792314486404\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377922931280548\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377922524490524\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037792194879341\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377921228962665\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377920389385564\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377919453398725\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377918442829293\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377917377682295\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377916275480588\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037791515164256\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037791401946743\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037791289158437\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377911782023372\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377910692142476\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377909627755626\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790859324937\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377907591897637\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790662596946\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377905696825285\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377904805161156\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377903951191167\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377903134642685\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377902354874045\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377901610750038\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790090148219\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377900225877\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377899582493058\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789896994516\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898386818765\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789783163085\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897303009376\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896799605297\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896320081255\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790480033524\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896508142666\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897076221765\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897566454695\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897979152313\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778983159049\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789857928791\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778987726903\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898900153726\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898966205006\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789897802261\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898938225987\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898851742966\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898723628634\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789855881323\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789836203232\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898137771835\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897890228915\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789762330699\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778973405869\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789704569321\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896741194492\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789642966561\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789611342638\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895794509184\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789547467897\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789515545913\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790549148533\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896261981596\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897624883334\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898908913163\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377900095319426\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377901172964865\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377902129214984\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790295264615\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377903638330523\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377904184518214\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790459218514\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790486717243\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377905016568474\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790504893617\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790497429538\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790480354898\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377904548048686\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377904219218905\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790382829424\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377903385832354\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377902901906824\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377902385723052\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377901845574296\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790128885299\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377900722037117\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790015161944\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377899584494537\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789902166079\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377898466258184\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789792096165\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789738786967\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789686857194\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896364243653\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789587569941\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789540345225\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894947832758\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894508718586\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894086149206\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893679791477\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789328934557\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892914509466\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789255486256\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778994725951\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789293943653\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789360005012\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894188391124\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789470202407\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789514007038\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789550306585\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895792790857\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896012073955\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896164611373\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896254693644\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896287112964\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896266949818\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896199423073\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377896089828984\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789594377911\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789576568874\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789556022685\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895331757987\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895084309694\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894821578444\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894546898325\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789426325355\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893973290138\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778936793365\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789338341157\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789308725539\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892792361125\n",
      "ACC: 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 2.037789249998859\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892211188144\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891926816796\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789164757382\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891374030788\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891106607655\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789084553975\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890591131957\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789034369066\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890103260405\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889869742103\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889643107823\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788942329325\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790351722791\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891085416677\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789306514102\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895115097893\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897197259793\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789926750527\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790127950158\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377903186740243\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790494482899\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377906514361874\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377907861937747\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377908963216638\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377909803523924\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377910378005057\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377910691585295\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037791075774044\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03779105968705\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377910234428662\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377909699100902\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790902053201\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377908228187533\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377907349942506\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377906411283373\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790543461354\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790443929117\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790344127326\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790245350246\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377901485819048\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037790054608875\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377899639958583\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789877119927\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897942127303\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377897153853057\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789640663855\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789570001527\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895032995426\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894404310446\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893812373418\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893255314232\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892731296754\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892238421973\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891774812533\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789133861435\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890928093434\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890541539454\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890177333047\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889833950102\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788951003576\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788920461354\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888916925957\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788864480455\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888387160694\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888142984855\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788791133929\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887691361627\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887482253696\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887283276603\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887093747975\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886913037084\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892029919864\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788742818538\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788809006294\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788871804912\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889304107026\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889842811876\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778903293237\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890759753323\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789113202978\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891445665206\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891701243023\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891900301317\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892045230275\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892139073457\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892185392907\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789218814473\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778921513269\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892079114095\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891975633995\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891844935814\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789169087892\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891517096005\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891327014894\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789112375256\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890910151852\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890688780953\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890461851336\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890231337856\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889998959056\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788976619441\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788953430477\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889304354575\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788907723022\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778888536544\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888634224357\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888419443\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888209615533\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888005005578\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788780579685\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887612138017\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788742405879\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887241568358\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887064637754\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788689320947\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886727198087\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886566831878\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778864120028\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788626222276\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788611734586\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885977217756\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889326662357\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788649155619\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788711918733\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788771380513\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888268601474\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888778067734\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788923783454\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788964501166\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788999788513\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890295988803\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789053972891\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890730509005\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789087054002\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890962664105\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789101020326\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891016778684\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789098622039\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778909224523\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890829374725\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789071079227\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890570350434\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890411507207\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789023744374\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789005122084\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788985533562\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889652204493\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788944387392\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889232184803\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889018738227\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788880491211\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888591879607\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888380628497\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888171979586\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788796660577\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788776505109\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887567737862\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887375034107\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788718719331\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887004341355\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788682658977\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788665398486\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886486536125\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886324215457\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886166967363\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788601471373\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788586735268\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778857247833\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885586876836\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885453494846\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778853244977\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885199738066\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893536379967\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886032619403\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887002598025\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887971603896\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888925380323\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788985497693\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890734775095\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891551375313\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789229258565\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892947930603\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893511013805\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893977421255\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789434581058\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789461746702\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789479583539\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894886448735\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894895855753\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789483175608\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894702462935\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789451655979\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778942827525\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894009027826\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893703069527\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789337199547\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789302227366\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892659797436\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892289319655\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789191509237\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789154070666\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789116915678\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890802905454\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890443838584\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890093428577\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889752836937\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788942288198\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788910411818\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888796879144\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888501315478\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888217622924\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788794578808\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788768531099\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887435915047\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887197272004\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788696901783\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886750825653\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788654227855\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778863428928\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886152251032\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788596997688\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885795628563\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885628809396\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885469136108\n",
      "ACC: 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 2.037788531624502\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885169919585\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788503118758\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884898218475\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884770683314\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788464828457\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788453074161\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884417789995\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789018555037\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788488047785\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885437381265\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885976089347\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886485271643\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788695891717\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788739224293\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788778164177\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888124607058\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788841967596\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888666692408\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888866006297\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889018877413\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889127302997\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788919390789\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889221731174\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889214094327\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788917448063\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788910643018\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788901344924\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788889893939\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888766218537\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888618285027\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888458034406\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788828803881\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888110515117\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887927479246\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788774073184\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788755190341\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887362328257\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887173129565\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788698596966\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886802133984\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886621159362\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886443554996\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886269723766\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886099975213\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788593453802\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885773572033\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788561717854\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788546539514\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885318181734\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788517559214\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788503756249\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884904017693\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884774868225\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884650010385\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884529328973\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788441270048\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884300136926\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884191380993\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884086316915\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883984782943\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883886648265\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788379177569\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788370003486\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788361129927\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778835254476\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883442362235\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883361924436\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883284022813\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788320854939\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883135404757\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883064490265\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882995710443\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892513089004\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883838563196\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884823802273\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778858749178\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886978373945\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788810544134\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778892323616\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890335037376\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891388720153\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377892369300663\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893255428208\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894029060197\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894676779005\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895189886583\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789556470965\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789580285587\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895910046444\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789589552545\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895771299133\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895551136427\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377895249738134\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894881830656\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789446158526\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377894002142365\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893515506313\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377893012119306\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789250087492\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377891989181443\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789148301833\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778909870806\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789050505245\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789003941305\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788959194645\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889163764666\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888755453304\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788836715242\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788799883903\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788764998232\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788731995412\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887007888895\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886713181517\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886434999644\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886172456097\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885924676047\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788569079547\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788546997355\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885261399897\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788506429728\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884877923584\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884701579823\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884535429898\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788437857659\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884229871515\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884088765774\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788395475119\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788382735541\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883706153552\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778835907178\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883480671906\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788337566485\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778832753714\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883179490253\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883087742465\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882999869246\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882915650902\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882834856\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882757274963\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882682708233\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882610977984\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788254191928\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788247537827\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882411212616\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788234929003\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882289487226\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788223168877\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882175787527\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882121683673\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882069284086\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887262349703\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788267559566\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883371791996\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788409663238\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884837957896\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885587287325\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886328904884\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788704790025\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788773084358\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788836516637\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888939977815\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889446285646\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788987743412\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789022924074\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890499939144\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789069021742\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789080294707\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789084272704\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890815774977\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037789072873272\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890589140137\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890404753054\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377890183225023\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788993188312\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377889657538435\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788936640799\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788906396765\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888754916493\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888443279746\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377888132425266\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887825206047\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778875236676\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377887229536755\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788694513339\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778866712596\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886407549197\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788615432749\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885911730185\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885679744336\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885458297893\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885247092675\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377885045832262\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884854187536\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884671789843\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788449820804\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788433303064\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884175835477\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377884026225073\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883883797527\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883748369805\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883619358266\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883496404596\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788337915946\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788326728055\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883160446095\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788305837557\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882960793587\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882867446897\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788277810445\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788269252427\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788261047623\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788253175691\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882456175738\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882383554367\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788231372585\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788224653403\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882181833065\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788211948605\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882059365118\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377882001350662\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788194533074\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788189119809\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377881988065374\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788233679226\n",
      "ACC: 0.2145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 2.037788284614629\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377883359808613\n",
      "ACC: 0.2145\n",
      "LOSS: 2.03778838719585\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788437463249\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788485623632\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788530953307\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788572847657\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788610806837\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886444610325\n",
      "ACC: 0.2145\n",
      "LOSS: 2.037788673544738\n",
      "ACC: 0.2145\n",
      "LOSS: 2.0377886979229243\n",
      "ACC: 0.2145\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ce83e8eb2215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-906464516b4e>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, epochs, lr, batch_size)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mY_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOSS:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-906464516b4e>\u001b[0m in \u001b[0;36m_forward_pass\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-1b3ff5670cc4>\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_W\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X=X_train, y=y_train, epochs=EPOCHS, lr=LR, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
